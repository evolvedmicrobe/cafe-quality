\documentclass[11pt]{article}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{fullpage}
\usepackage{minted}
\usepackage{float}
\usepackage{enumerate}
\usepackage{amsmath}
\usepackage{cite}
\usepackage{float}
\usepackage{pdflscape}

\newfloat{algorithm}{t}{lop}
\renewcommand{\thefigure}{S\arabic{figure}}
\renewcommand{\thetable}{S\arabic{table}}

\title{Supplementary Note 1 to Chin et al. ``Non-Hybrid, Finished Microbial Genome Assemblies from Long-Read SMRT Sequencing Data''}
\author{}
\date{}
\begin{document}
\maketitle

\tableofcontents 

\section{DAGCon: A Directed Acyclic Graph Based Consensus Algorithm}

We describe an algorithm of constructing directed acyclic graph from existing pairwise alignments used for the pre-assembly step in the current implementation for the hierarchical genome assembly process. The algorithm presented here is motived by the original idea representing multiple sequence alignment as a directed acyclic graph proposed by Lee \cite{Lee01032002}. In Lee's paper, the input data is just a collection of sequences and the partial order alignment (POA) is constructed by aligning the POA graph to a new added sequence in each iteration.  In our applications, we usually know about a candidate or a reference sequence and the goal is to identify differences between sequencing reads and the reference sequence. The alignments between the sequence reads and the reference are usually available from the earlier stages of the data processing pipeline by high performance alignment software to map the reads to the reference.  Such alignment data between the reads and a reference is the input to the algorithm and is used to construct a directed acyclic graph denoted as the sequence alignment graph (SAG). A SAG summarize all alignments from the reads to the reference sequence and is used to use to generate a consensus sequence that may be different from the reference. 

Under the assumptions that (1) the majority of variants are substitutions and (2) the sequencing errors are usually mis-matches, a variant can be called by easily counting the read bases mapping the a particular position in the reference sequence and a consensus sequence can be constructed by using the variant call and the reference sequence. When the characteristics sequencing data does not satisfy the assumptions (1) and (2), e.g. when insertions and deletions instead of mismatches become the dominant error mode, it is necessary to go beyond counting the mismatches of each reference base. Instead, a multiple sequence alignment (MSA) is constructed and used to call variants and build the consensus sequence. Complete optimized MSA construction is usually computationally expensive. Most MSA construction algorithms reported in the literature are mostly focusing on constructing a MSA within the context that there exist some evolution relations among the sequences \cite{Edgar01032004}\cite{Larkin01112007}.  This may not be applicable to DNA sequencing application where the difference among the reads are due to random independent errors caused by the sequencing reaction rather than some biological changes during the course of evolutional history.  It is therefore desirable to constructed a MSA to reflect such property of the sequencing read data. For example, a consistency based multi-read alignment algorithm was developed for short read assembly by Rausch et. al.\cite{Rausch01052009}. The construction of the SAG described here uses only the alignments of the reads and the references such that all reads are treated equivalently in constructing the MSA. This is more suitable for generating consensus where all reads should be treated equally.

\subsection{Outline of the SAG construction algorithm}

The input of the algorithm is the collections $(\{a_i, r_i\}|_{i=1\cdots N}, R)$ where $a_i$ is the alignment of a read $r_i$ to a reference sequence $R$.  The output is a graph $G=\{V,E\}$ where $V$ is the nodes labeled by the DNA bases A,C,G or T and $E$ are the edges that represent the connection of the consecutive bases in the reads.  Each read can be represented as a unique path in graph $g_r=\{v_r, e_r\}$.  Each node has an associated set of reads $r_v=\{r|v \in g_r\} $ that pass through it. Each edge also has an associated set $r_e=\{r|e \in g_r\}$ of reads that pass through it.

Here is the outline of the algorithm for constructing the SAG
\begin{enumerate}
\item Normalize mismatches and degenerated alignment positions as shown in figure \ref{fig:shift_gap}
\item Setup an initial graph from the reference sequence as shown in figure \ref{fig:DAGCon}(a) 
\item Construct a multigraph (a graph that permits multiple edges connecting the same two nodes) from the alignments and merge the edges that are connecting the same nodes as shown in figures \ref{fig:DAGCon}(b) to \ref{fig:DAGCon}(d)
\item According to the topological order of the node in the graph, systematically (1) merge nodes that have the same label and are connected to the same node and (2) merge nodes that have the same label and are connected from the same node as shown in figures \ref{fig:DAGCon}(d) to \ref{fig:DAGCon}(f)  
\end{enumerate}


\begin{figure}[h]
\subfloat[Change mismatches into indels] {
\tt
\begin{tabular*}{8cm}{ r l l l }
  read: & CAC &               & C-AC \\
        & | | & $\rightarrow$ & |\ \ | \\
   ref: & CGC &               & CG-C 
\end{tabular*}
}
\subfloat[Shift the equivalent gaps to the right in the reference] {
\tt
\begin{tabular*}{8cm}{ r l l l }
  read: & CAACAT &               & CAACAT \\
        & |\ |\ || & $\rightarrow$ & |||\ \ | \\
   ref: & C-A-AT &               & CAA--T 
\end{tabular*}
}

\subfloat[Shift the equivalent gaps to the right in the read] {
\tt
\begin{tabular*}{8cm}{ r l l l }
  read: & -C--CGT &               & CCG--T \\
        & \ |\ \ | | & $\rightarrow$ & |||\ \ | \\
   ref: & CCGAC-T &               & CCGACT 
\end{tabular*}
}

\caption{Examples of normalization of gaps in the alignment}
\label{fig:shift_gap}
\end{figure}


\begin{figure}

\centering
\includegraphics[width=6in]{DAGCon.pdf}
\caption{(a) A backbone linear graph $G_{\rm B}$. The special nodes ``Begin'' and ``End'' are not shown.
         (b) The sequence alignment graph before the edges are merged when the first alignment is added to the group.
         (c) The intermediate sequence alignment graph $G_{\rm I}$ after the first alignment is added. The numbers on those directed edges indicate the number of ``reads'' (including the backbone) that pass through them.
         (d) The intermediate sequence alignment graph $G_{\rm I}$ after the 4 alignments are added. The initial pairwise alignments between the reads to the backbone reference sequence are shown on the right. The nodes labelled with `a', `b', and `c' can be further merged according the merging rules.
         (e) The final sequence alignment graph $G_{\rm S}$ after node merging.
         (f) The consensus path of $G_{\rm S}$ is highlighted with the thicker directed edges and the thicker circles.}
\label{fig:DAGCon}
\end{figure}


\subsection{Normalize mismatches and degenerated alignment positions}

When representing an alignment as a directed graph, each node only has one label which is the base that aligned with the node. Mismatches between two sequences are indistinguishable from a nearby insertion-deletion pair. To simplify the construction of the SAG, one can convert all mis-matches to local insertion-deletion pairs in the alignment. Moreover, there are cases where there are multiple ways to place gaps in an alignment as shown in figure \ref{fig:shift_gap}.  As an optional step, to ensure getting consistent results of the SAG, we normalize the alignment by moving all gaps to the right most equivalent positions as shown in figure \ref{fig:shift_gap}. One reason for normalizing the gap placement in the alignments is to take into account somewhat arbitrary rules for resolving alignment degeneracy reported by an aligner. For example, if a read is aligned in a reversed orientation first, the result could be different from aligning the read in the forward direction and then reverse the final alignment string after the alignment process. A procedure to normalize the placement of the gaps will reduce such potential inconsistency and improve the final consensus accuracy.


\subsection{Setup an initial graph from the reference sequence}

The first step to construct an alignment graph is to use the reference sequence to build a backbone linear graph $G_{\rm B}$ that simply connects each base in the reference within two special nodes: ``Begin'' and ``End'' nodes. Namely the initial backbone graph consists of the nodes (1) $v_i$ for $i \in [0, |R| -1]$, $v_{\rm B}$, and $v_{\rm E}$ where
${\rm label}(v_i) = R_i$, where  $R_i$ indicates the base label of the reference at position $i$ and the edges $e_{i,j} : v_i \rightarrow v_{j=i+1}$ for $i \in [0, |R|-2]$ , $e_{{\rm B} 0}: v_{\rm B} \rightarrow v_0$ and $e_{{|R|-1},{\rm E}}: v_{|R|-1} \rightarrow v_{\rm E}$.  

\subsection{Construct the multigraph from the alignments and merge the edges}

Conceptually, the multigraph is constructed iteratively when a new alignment represents a list of pair of bases and indel symbol $A = \{(q_i, r_i)| q_i,r_i \in {\rm A,C,G,T,-}\}$ is added into the graph. We start from the $v_{\rm B}$ node as the initial node, and scan through each alignment position between a read and the reference. When there is a match between the read and the reference, we connect the previous node to the reference node in the backbone graph. If there is an insertion in the read, we create a new node labelled by the read base, and a new edge pointing from the previous node to the new node is created.  Once all alignments are added, we merge all edges that are connecting the same two nodes into a single edge to convert the multigraph into a regular graph.  In fact, the merge step can be integrated at the time when the alignment is added. The pseudo-code for constructing the intermediate graph $G_{\rm I} = (V_{\rm I}, E_{\rm I})$ is listed in Algorithm \ref{algGI}.

\begin{algorithm}[H]
\caption{Generate $G_{\rm I}$ from the alignments}          
\label{algGI}
\begin{algorithmic}[1]
\State $G_{\rm I}\gets G_{\rm B}$
\State $v_{\rm last} \gets v_{\rm B}$
\For {$p = 0 \dots |A|$}
	\State $q, r \gets q_p, r_p$
	\If {$q = r$ and $q \neq {\rm ``-"}$ and $r \neq {\rm ``-"}$ } 
		\State $v_{r_p} \gets \textrm{the node corresponding the reference base } r \textrm{ at alignment position } p $
		\If  {$(e: v_{\rm last} \rightarrow v_{r_p}) \in E_{\rm I}$}
			\State $\textrm{weight}(e) \gets \textrm{weight}(e) + 1$
		\Else
			\State create a new edge $(e: v_{\rm last} \rightarrow v_{r_p})$, and add it into $G_{\rm I}$
		\EndIf
		\State $v_{\rm last} \gets v_{r_p}$
	\ElsIf {$q \neq {\rm ``-"}$ and $r = {\rm ``-"}$}
		\State create a new node $v_{q_p}$
		\State ${\rm lable}(v_{q_p}) \gets q$
		\State crate a new edge $e: v_{\rm last} \rightarrow v_{q_p}$
		\State add $e$ and $v_{q_p}$ into $G_{\rm I}$
		\State $v_{\rm last} \gets v_{q_p}$
	\EndIf
	\If  {$(e: v_{\rm last} \rightarrow v_{\rm E}) \in E_{\rm I}$}
		\State $\textrm{weight}(e) \gets \textrm{weight}(e) + 1$
	\Else
		\State create a new edge $(e: v_{\rm last} \rightarrow v_{\rm E})$, and add it into $G_{\rm I}$
	\EndIf	
\EndFor
\end{algorithmic}
\end{algorithm}  

\subsection{Merge nodes}
The next step to reduce the complexity of $G_{\rm I}$ is to find those nodes which are similar to each other in a local context and merge them in a way that preserves the directed acyclic property of the graph.  This is also analogous to a greedy local alignment for the reads with the same base from them before and after an alignment node. The general idea of the algorithm to generate the final sequence alignment graph $G_{\rm S}=(V_{\rm S}, E_{\rm S})$ is to define an order to go through the graph and merge the nodes according to their labels and connection to the other nodes. For example, we merge all nodes labeled by the same label and are connected to the same out-node, and we also merge all nodes with the same label that are connected from the same in-node.  The algorithm presented here is based on a variant of the topological sorting algorithm. The pseudocode is listed as Algorithm \ref{algGS},\ref{algGSMergeIn},\ref{algGSMergeOut}.  An example of showing the process is shown in figures \ref{fig:DAGCon}(d) to (f).

\begin{algorithm}[H]

\caption{Generate $G_{\rm S}$ from $G_{\rm I}$}          
\label{algGS}
\begin{algorithmic}[1]
\Function {in\_edges}{$v$}
	\State \Return $\{e|u \in V_{\rm S}, (e:u\rightarrow v) \in E_{\rm S} \} $
\EndFunction
\Function {out\_edges}{$v$}
	\State \Return $\{e|u \in V_{\rm S}, (e:v\rightarrow u) \in E_{\rm S} \} $
\EndFunction
\Function {out\_nodes}{$v$}
	\State \Return $\{u|u \in V_{\rm S}, (e:u\rightarrow v) \in \Call{in\_edges}{v} \} $
\EndFunction
\Function {in\_nodes}{$v$}
	\State \Return $\{u|u \in V_{\rm S}, (e:v\rightarrow u) \in \Call{out\_edges}{u} \} $
\EndFunction

\State $ G_{\rm S}\gets G_{\rm I} $
\ForAll {$e \in G_{\rm I}$ }
	\State ${\rm visited}(e) \gets False$
\EndFor
\State $s \gets \textrm{an empty FIFO queue}$
\For { $v \in V_{\rm S}$ }
	\If { $|$ \Call{in\_edges}{$v$} $| = 0 $ }
		\State push $v$ into $s$
	\EndIf
\EndFor 
\While{$|s| \neq 0$}
	\State $v \gets \textrm{pop}(s)$
	\State \Call{merge\_in\_node}{$v$}
	\State \Call{merge\_out\_node}{$v$}
	\ForAll { $e \in \Call{out\_edges}{v}$}
		\State $ {\rm visited}(e) \gets True $
	\EndFor
	\ForAll { $u \in \Call{out\_nodes}{v} $ }
		\If { $|\{ e | e\in \Call{in\_edges}{u}, \textrm{visited}(e) = False \} | = 0 $}
			\State push $u$ into $s$
		\EndIf
	\EndFor
\EndWhile
\end{algorithmic}
\end{algorithm}  


\begin{algorithm}[H]
\caption{Merge in-nodes for the node $v$}          
\label{algGSMergeIn}
\begin{algorithmic}[1]
\Function {merge\_in\_node}{$v$}
\For { $l \in \{\textrm{A, C, G, T}$\} }
	\State $U_l \gets \{u|u \in \Call{in\_nodes}{v}, |\Call{out\_edge}{u} | = 1, \textrm{label}(u) = l\}$ 
	\State $\textrm{label}(u_l) \gets l$
	\State add $(e:u_l \rightarrow v)$ to $G_{\rm s}$
	\State $\textrm{visited}(e:u_l \rightarrow v) \gets True$
	\ForAll { $u \in U_l$ }
		\State $\textrm{weight}(e:u_l \rightarrow v) \gets \textrm{weight}(e:u_l \rightarrow v) + \textrm{weight}(e:u \rightarrow v)$
		
		\ForAll { $u^\prime \in \Call{in\_nodes}{u}$ }
			\State add $(e:u^\prime \rightarrow u_l)$ to $G_{\rm s}$
			\State $\textrm{visited}(e:u^\prime \rightarrow u_l) \gets True$
			\State $\textrm{weight}(e:u^\prime \rightarrow u_l) \gets \textrm{weight}(e:u^\prime \rightarrow u_l) +  \textrm{weight}(e:u^\prime \rightarrow u)$
			\State remove $(e:u^\prime \rightarrow u)$ from $G_{\rm s}$ 
		\EndFor
		\State remove $u$ from $G_{\rm s}$ 
	\EndFor 
	\State add $u_l$ to $G_{\rm s}$
	\State \Call{merge\_in\_node}{$u_l$} \Comment{Recursively merge upstream nodes}
\EndFor
\EndFunction
\end{algorithmic}
\end{algorithm}  

\begin{algorithm}[H]
\caption{Merge out-nodes of the node $v$}          
\label{algGSMergeOut}
\begin{algorithmic}[1]
\Function {merge\_out\_node}{$v$}
\For { $l \in \{\textrm{A, C, G, T}$\} }
	\State $U_l \gets \{u|u \in \Call{out\_nodes}{v}, |\Call{in\_edge}{u} | = 1, \textrm{label}(u) = l\}$
	\State $\textrm{label}(u_l) \gets l$
	\State add $(e:v \rightarrow u_l)$ to $G_{\rm s}$
	\State $\textrm{visited}(e:v \rightarrow u_l) \gets True$
	\ForAll { $u \in U_l$ }
		\State $\textrm{weight}(e:v \rightarrow u_l) \gets \textrm{weight}(e:v \rightarrow u_l) + \textrm{weight}(e:v \rightarrow u_l)$
		\ForAll { $u^\prime \in \Call{out\_nodes}{u}$ }
			\State add $(e:u_l \rightarrow u^\prime)$ to $G_{\rm s}$
			\State $\textrm{visited}(e:u_l \rightarrow u^\prime) \gets True$
			\State $\textrm{weight}(e:u_l \rightarrow u^\prime) \gets \textrm{weight}(e:u_l \rightarrow u^\prime) + \textrm{weight}(e:u \rightarrow u^\prime)$
			\State remove $(e:u \rightarrow u^\prime)$ from $G_{\rm s}$ 
		\EndFor
		\State remove $u$ from $G_{\rm s}$ 
	\EndFor
	\State add $u_l$ to $G_{\rm s}$
	\State \Call{merge\_out\_node}{$u_l$} \Comment{Recursively merge downstream nodes}
\EndFor
\EndFunction
\end{algorithmic}
\end{algorithm}  

\subsection{A Python implementation}

Here we show a short code snippet on building $G_{\rm S}$ with a python implementation.

\begin{minted}[mathescape,
               linenos,
               numbersep=5pt,
               gobble=2,
               frame=lines,
               framesep=2mm]{python}
                  
  from aligngraph.aligngraph import AlnGraph
  backbone_seq = "ATATTAGGC" #the backbone_seq is the reference sequence
  aln_graph = AlnGraph(backbone_seq) 
  # each alignment is represented as (q_start, q_end, q_aln_seq),(r_start, r_end, r_aln_seq),
  alignments = [((0,  9, 'A-TAGCCGGC'),   
                 (2,  9, 'ATTA---GGC')), 
                ((0, 10, 'ATA-TACCGAG-'), 
                 (0,  9, 'ATATTA--G-GC')), 
                ((0, 10, 'ATCATCC--GGC'), 
                 (0,  9, 'AT-AT--TAGGC')), 
                ((0,  9, 'ATA-TACGGC'),   
                 (0,  9, 'ATATTA-GGC'))]  
  for aln in alignments:
      aln_graph.add_alignment( aln ) #add alignment information to graph to construct $G_{\rm I}$
  aln_graph.merge_nodes() #convert $G_{\rm I}$ to $G_{\rm S}$
  s, c = aln_graph.generate_consensus() #generate consensus sequence from $G_{\rm S}$
\end{minted}

\subsection{Generate consensus sequence from $G_{\rm s}$}

The consensus algorithm is listed in Algorithm \ref{consensus} and \ref{consensus2}. The basic idea for generating a consensus is (1) assigning a score for each node and (2) find a path that maximizes the sum of the score of the nodes in the path. Here we calculate the score of a node by checking the weight of the all out-edges of the node. If an edge takes more than half of the local coverage (which is equal to the maximum weight that an edge can have around the node), it gets a positive score. Otherwise, it gets a negative score. The score of any path in the graph is simply defined as the sum of the scores of the nodes in the path. The consensus is then built by finding the path that has the maximum path score in all possible paths. Since the graph is directed acyclic, such path can be found by simple dynamic programming and backtracking. The algorithms shown in Algorithm \ref{consensus} and \ref{consensus2} perform the topological sorting and calculate the best score and the best edge of each node at the same time. Additionally, backtracking is done to construct the best consensus path in the group. 

\begin{algorithm}[H]
\caption{Find the best path in $G_{\rm s}$ as the consensus sequence}          
\label{consensus}
\begin{algorithmic}[1]
\State $s \gets \textrm{an empty FIFO queue}$
\ForAll {$v \in V_{\rm s}$}
	\If {$|\Call{out\_edges}{v}| = 0$}
		\State put $v$ into $s$
		\State $\textrm{best\_out\_edge}(v) \gets \emptyset$ 
		\State $\textrm{best\_in\_edge}(v) \gets \emptyset$
		\State $\textrm{node\_score}(v) \gets 0$  
	\EndIf
\EndFor	
\ForAll {$e \in E_{\rm s}$}
	\State $\textrm{visited}(e) \gets False$
\EndFor	
\While{$|s| \neq 0$}
	\State $v \gets \textrm{pop}(s)$
	\State $e_{\rm best} \gets \emptyset$
	\State $score_{\rm best} \gets \emptyset$
	\For { ($e_{\rm out}:v \rightarrow u) \in \Call{out\_edges}{v}$} \Comment{find the best out-edge for node $v$}
		\State $score \gets \textrm{node\_score}(u)$
		\State $coverage \gets  \textrm{the local coverage for the node } u$
		\If{$u \in V_{\rm B}$}
			\State $score_{\rm new} \gets score - \delta$ \Comment{ choose $\delta$ (default $\delta = 10$) to make the path along the reference  unfavorable} 
		\Else
			\State $score_{\rm new} \gets \textrm{weight}(e_{\rm out}) - coverage * 0.5 + score$
		\EndIf
		\If{$score_{\rm best}$ is $\emptyset$ or $score_{\rm new} > score_{\rm best}$}
			\State $e_{\rm best} \gets e_{\rm out}$
			\State $score_{\rm best} \gets score_{\rm new}$
		\EndIf
	\EndFor
	\If{$e_{\rm best} \neq \emptyset$}
		\State $\textrm{best\_score\_edge}(v) = e_{\rm best}$
		\State $\textrm{node\_score}(v) \gets score_{\rm best}$ 
	\EndIf
	\For {$e \in \Call{in\_edges}{v}$}
		\State $\textrm{visited}(e) \gets True$	
	\EndFor
	\ForAll { $u \in \Call{in\_nodes}{v} $ }
		\If { $|\{ e | e\in \Call{out\_edges}{u}, \textrm{visited}(e) = False \} | = 0 $}
			\State push $u$ into $s$
		\EndIf
	\EndFor
\EndWhile
\algstore{consensus}
\end{algorithmic}
\end{algorithm}  

\begin{algorithm}[H]
\caption{Find the best path in $G_{\rm s}$ as the consensus sequence (cont.)}          
\label{consensus2}
\begin{algorithmic}[1]
\algrestore{consensus}
\State $v \gets v_{\rm B}$
\State $consensus\_path \gets \textrm{empty list}$
\While{$True$}
	\State append $v$ to $consensus\_path$
	\If {$v$ has no best score edge}
		\State Break
	\Else
		\State ($e_{\rm best\_out}:v \rightarrow u) \gets \textrm{best\_score\_edge}(v)$
		\State $v \gets u$
	\EndIf
\EndWhile
\end{algorithmic}
\end{algorithm}  

Such consensus algorithm is typically ``biased'' toward to the reference used in the alignment. If the real sample sequences have high variation from the reference sequence, then there will be opportunity for further improvement from the single-pass consensus algorithm.  One way to improve it is to use an iterative process to increase the final quality of the consensus sequence. Namely, one can take the consensus sequence as the reference sequence and align the reads back to the first iteration of the consensus sequence and repeatedly apply the consensus algorithm to generate the next iteration of the consensus sequences.  Such iteration process can reconstruct the regions that are very different from the original reference, although it is generally more computationally intensive. The other possible strategy is to identify the high variation region, then, rather than using the original reference, one uses one of the reads as the reference to reconstruct those regions to avoid the ``bias'' toward the original reference. 

\section{Using DAGCon Algorithm to Generate Pre-assembled Reads}

We first mapped all subreads to the long seeding set using blasr. We split the target seeding sequences into chunks for mapping other reads to them in parallel. In such case, we use the follow blasr options for mapping:  ``{\tt -nCandidates 50 -minMatch 12 -maxLCPLength 15 -bestn 5 -minPctIdentity 70.0 -maxScore -1000 -m 4}''.  After mapping, the blasr output files are parsed for each chunk and top 12 hits are selected for each query reads as default. The number of top hits could impact the quality of the pre-assembly reads and the ability to resolve very similar repeats. 

For each seed read, we collect the all subreads aligned to it from the alignment output of the blasr program. Each subread are trimmed according to the alignment coordinates. For example, if blasr reports base $s_1$ to $s_2$ are aligned to the seed read, we will use base $s_1+\Delta$ to $s_2-\Delta$ for pre-assembly. We choose $\Delta$ to be 100 since the blasr aligner sometimes generate poor alignment at the ends such that chimera reads might still have good coverage across the chimeric junction. More aggressive trimming with a larger $\Delta$ will help to remove all chimeric reads at a cost losing useful bases. One can avoid losing base by choosing smaller $\Delta$.

The trimmed subreads are then used with the seed read to generate the pre-assembled read with the DAGCon algorithm described in the previous section.

A benchmark run was performed with Amazon EC2 cloud computing service. It took about 1.5 hours on a virtual cluster of 8 {\tt c1.xlarge} instances (8 virtual cores with 2.5 EC2 Compute Units each instance) for the mapping step between the reads using {\tt blasr} and 7 hours for the DAGCon step in the same virtual cluster for the 8 SMRTCell E. coli dataset.  The pure python based DAGCon code can be accelerated by declaring the type of each variable to reduce the dynamical typing resolution overhead with Cython (http://www.cython.org).  A test version of DAGCon implemented with Cython from the same code base used only 1 hour in the same cluster for the pre-assembly step.
The detailed steps on how to set the running environment can be found at \\
{\tt http://pacb.com/devnet/files/software/hgap/HGAP\_README.rst} and \\
{\tt http://pacb.com/devnet/files/software/hgap/Starcluster\_instruction.rst}.

\section{Alternative Pre-assembly Method Using AMOS}

To demonstrate the generality of the HGAp methodology, we demonstrate that we can also use BLASR and tools from the AMOS assembly toolkit as part of the 1.4 Pacific Biosciences SMRT Analysis software package to test it on the Pedobacter heparinus genome. 

In this process, all continuous long reads from {\it E. coli} (8 SMRT cells),  {\it M. ruber} (4 SMRT cells), and {\it P. heparinus} (7 SMRT cells) genomes were aligned to those reads of length greater than 5kb using BLASR with the following options:  

\begin{verbatim}
-minReadLength 200 -maxScore -1000 -bestn 24 -maxLCPLength 16 -nCandidates 24
\end{verbatim}

These alignments were used to arrange reads in AMOS layout messages, which were uploaded to an AMOS bank along with the read sequences and quality values. Importantly, the layouts contained only the portion of each read that aligned to that particular CLR seed read, trimmed by 50bp on each side of the alignment. Reads were kept in the same layout if these trimmed aligned portions overlapped by at least 100bp, otherwise they were split into multiple layouts. Consensus sequence was generated using the off-the-shelf AMOS executable make-consensus with the following option: -L.

The resulting consensus sequences were trimmed to regions with greater than 59.5 average QV (as called by the make-consensus algorithm) and greater than 500bp in length. These pre-assembled reads were evaluated and further assembled into contigs using the Celera Assembler and corrected using Quiver in a manner identical to the other pre-assembled read data sets in Supplementary Table 1. The software used to perform the read pre-assembly will be available as a command line tool and a SMRT Portal protocol in the 1.4 release of Pacific Biosciences SMRT Analysis software. The SMRT Pipe parameter file necessary for regenerating these results is included here:

\begin{minted}[mathescape,
               linenos,
               numbersep=5pt,
               gobble=2,
               frame=lines,
               framesep=2mm]{xml}
 \<smrtpipeSettings>
    <module name="P_Fetch"/>
    <module name="P_Filter">
        <param name="minLength">
            <value>50</value>
        </param>
        <param name="minSubReadLength">
            <value>500</value>
        </param>
        <param name="readScore">
            <value>0.80</value>
        </param>
    </module>
    <module name="P_FilterReports"/>
    <module name="P_ErrorCorrection">
        <param name="useFastqAsShortReads">
            <value>False</value>
        </param>
        <param name="useFastaAsLongReads">
            <value>False</value>
        </param>
        <param name="useLongReadsInConsensus">
            <value>False</value>
        </param>
        <param name="useUnalignedReadsInConsensus">
            <value>False</value>
        </param>
        <param name="useCCS">
            <value>False</value>
        </param>
        <param name="minLongReadLength">
            <value>5000</value>
        </param>
        <param name="blasrOpts">
            <value> -minReadLength 200 -minPctIdentity 75 -maxScore -100 -bestn 24 
                    -maxLCPLength 16 -nCandidates 24 </value>
        </param>
        <param name="consensusOpts">
            <value> -L </value>
        </param>
        <param name="layoutOpts">
            <value> --overlapTolerance 100 --trimHit 50 </value>
        </param>
        <param name="consensusChunks">
            <value>60</value>
        </param>
        <param name="trimFastq">
            <value>True</value>
          </param>
        <param name="trimOpts">
            <value> --qvCut=59.5 --minSeqLen=500 </value>
        </param>
    </module>
 \</smrtpipeSettings>
\end{minted}

The system requirements and installation instructions for the SMRTPipe V.1.4 software can be found at
{\tt https://github.com/PacificBiosciences/SMRT-Analysis/wiki/\\
SMRT-Analysis-Software-Installation-(v1.4.0)}. A benchmark run was performed using the P. heparinus dataset (7 SMRT Cells) on the PacBio internal cluster (consisting of 27 nodes, ranging from 16-24 processing units with 32GB of RAM).  Pre-assembly took 2 hours and 16 minutes (wall time) using 69 core$\times$cpu hours, Celera Assembler took 3 hours and 50 minutes (wall time), while the re-sequencing (including quiver refinement) stage took 63 minutes (wall time) using 10.78 core$\times$cpu hours.
 
Detailed step-by-step instructions about how to execute pre-assembly module with the SMRTAnalysis v.1.4 are at \\
{\tt https://github.com/PacificBiosciences/Bioinformatics-Training/wiki/HGAP} and \\
{\tt https://github.com/PacificBiosciences/Bioinformatics-Training/tree/master/hgap}

\section{Quiver Consensus Algorithm}

The Quiver algorithm was originally developed for Pacific Biosciences' Circular Consensus Sequencing (CCS) analysis mode, and is now available for multimolecule consensus analyses.  With coverage levels of 60x, we have found that Quiver has reliably achieved accuracies exceeding Q50 (99.999\%) in a number of de novo assembly projects.

We describe the details of the Quiver algorithm below.

\subsection{Outline of the algorithm}

Given a vector of reads $\mathbf{R}$ from a single (unknown) template $T$, Quiver uses a greedy algorithm to maximize the likelihood $\Pr(\mathbf{R} \mid T)$ for the unknown $T$. We develop a likelihood function $\Pr(\mathbf{R} \mid T)$ which encodes the sequencing error model and is specific to a particular sequencing chemistry and enzyme.  The parameters within the model are derived using a training step that learns an error model from SMRT sequencing data on a known template. This is performed in-house at Pacific Biosciences.

For a long reference, we process the consensus with tiling windows across the reference to limit the amount of memory used. Here is a rough sketch of the $\mathrm{QuiverConsensus}$ algorithm for reference window $W$:\begin{enumerate}
   \item Use reference alignment to identify reads $\mathbf{R}=\{R_1, R_2, \ldots R_K\}$ corresponding to $W$
   \item Create a candidate template sequence using $\hat{T_1} \leftarrow \mathrm{POAConsensus}(\mathbf{R})$. (Note that the original reference is not used as the candidate template sequence.)
   \item Repeat until convergence: $$\hat{T}_{s+1} \leftarrow \hat{T_{s}} + \mu$$ where $\mu$ is a single base mutation to the template (any possible single-base substitution, insertion, or deletion) that increases the likelihood:
     $$\Pr(\mathbf{R} \mid \hat{T_s} + \mu) > \Pr(\mathbf{R} \mid \hat{T_s})$$

\end{enumerate}

\subsection{Pulse metrics}

In addition to basecalls, the basecaller software includes metrics
reflecting its confidence against the various types of errors.  The
pulse metrics, or \emph{QV} features, are encoded using the standard
Phred-scaling convention,

$$QV = -10 \log_{10} p_{error}.$$

The pulse metrics we use are as follows:
\begin{enumerate}
   \item \textsf{InsertionQV}, \textsf{SubstitutionQV}: Probability
     that this base call is actually an insertion (substitution)
     relative to the true template.
   \item \textsf{DeletionQV}: Probability that the basecaller omitted
     a base relative to the true template, \emph{prior} to this
     basecall.  The maximum likelihood missed base is encoded (as an
     ASCII character) in the \textsf{DeletionTag} feature.
   \item \textsf{MergeQV}: Probability that the basecaller merged
     together two identical adjacent template bases into this
     basecall.
\end{enumerate}

\subsection{Computing the template likelihood function, $\Pr(\mathbf{R} \mid T)$}
Reads are assumed independent, so we have
$$\Pr(\mathbf{R} \mid T) = \prod_{k=1}^{K}\Pr(R_k \mid T)$$

In PacBio reads, indel errors are the dominant mode, so the model needs to consider the possible \emph{alignments}---the ways $T$ can be construed to have generated $R_k$:
$$\Pr(R_k \mid T) = \sum_\mathcal{A} \Pr(R_k, \mathcal{A} \mid T) $$

Where $\mathcal{A}$ represents the alignment between the read and template sequences. This summation can be computed efficiently using a standard Sum-Product dynamic programming approach.  In practice we use the Viterbi algorithm, which maximizes over the unknown alignment $\mathcal{A}$ rather than summing. Our experience has shown the Viterbi algorithm to be substantially more computationally efficient with no impact on accuracy.

\subsection{Sketch of dynamic programming}
The Sum-Product recursions build up the forward matrix $A$ and backward matrix $B$ of probabilities defined by
\begin{align}
  A_{ij} \doteq & \text{ marginal prob. of an alignment of $R[0..(i-1)]$ to $T[0..(j-1)]$}, \text{ and } \\
  B_{ij} \doteq & \text{ marginal prob. of an alignment of $R[i:I-1]$ to $T[j:J-1]$}
\end{align}
where $S[p..r]$ denotes the substring $S_p S_{p+1} \cdots S_r$.

The recursions are computed as
\begin{align}
   A_{ij} &= \sum_{m: (i',j') \to (i, j)}   (A_{i'j'} \times \mathrm{moveScore}(m)), \text{ and } \\
   B_{ij} &= \sum_{m: (i, j)  \to (i', j')} (\mathrm{moveScore}(m) \times B_{i'j'})
\end{align}

For the Viterbi algorithm, replace \emph{marginal} by \emph{maximum}
and \emph{sum} by \emph{max} in the above definitions.

\subsection{Alignment moves}
The moves $m$ considered in the above alignment recursions are the familiar Smith-Waterman alignment moves \textsf{Incorporate}, \textsf{Delete}, and \textsf{Extra}, and additionally a \textsf{Merge} move that is only available when $T[j] = T[j-1]$.  The \textsf{Merge} move allows us to model the pulse merging process independently from the spontaneous deletion process; the addition of this move has consistently shown improved accuracy results.

\begin{figure}[H]
\centering
\includegraphics[width=3in]{moves.pdf}
\caption{The alignment moves available in Quiver's probability model
  $Pr(R \mid T)$.  In addition to the standard Smith-Waterman moves, a
  \textsf{Merge} move is used to model the process of pulse merging in
  homopolymer regions.}
\label{fig:Quiver-alignment-moves}
\end{figure}


\subsection{Efficiently computing $\Pr(R_k \mid T + \mu)$ }
The main loop of Quiver requires testing the likelihood of all possible single-base point mutations to a template.  We represent a single base mutation $\mu$ to a template sequence $T$ as $T + \mu$. It is essential that we can compute $\Pr(R_k \mid T + \mu)$ very rapidly, given the $A$ and $B$ matrices used to compute $\Pr(R_k \mid T)$.

To do so we exploit the ``forward-backward trick'', which in our notation amounts to the identity
\begin{align}
  \Pr(R \mid T) =& A_{IJ} = B_{00} \nonumber \\ 
     =& \max_{m: (i',j') \to (i, j)} A_{i'j'} \times B_{ij},
     \text{ for \bf{any} $j$}, \label{eqn:forward-backward} 
\end{align}
so that the likelihood can be computed directly from two consecutive columns of the $A$ matrix and the following column of the $B$ matrix. Modification of the template via a single base substitution $\mu$ at
postition $j$ induces new forward-backward matrices $A'$ and $B'$, where only columns $\{j+1, \ldots, J\}$ of $A$ and $A'$, and columns $\{0, 1, \ldots, j\}$ of $B$ and $B'$, disagree.  To compute $\Pr(R \mid T + \mu)$, we thus need only compute columns $j+1$ and $j+2$ of $A'$, and use them together with column $j+3$ of $B'$ (which can be taken directly from $B$) in Equation~\ref{eqn:forward-backward} to compute the likelihood.  An analogous calculation is performed in the case of insertions and deletions.

Using this optimization, computing the likelihood for each single-base point mutation requires $O(L)$ time and space, naively, where we use $L$ to denote the template length.  Using the further banding optimization described below, the space and time requirements are both reduced to $O(1)$.


\subsection{Banding optimization}
To reduce the computational time and storage requirements to manageable levels, the dynamic programming algorithm is approximated using a banding heuristic.  In short, rather than computing full columns of the forward and backward matrices, we only compute a narrow band of high-scoring rows within each column.  This approach is commonplace in modern aligners.  Adopting this heuristic reduces the runtime for the initial computation of $A$ and $B$ to $O(L)$ (down from $O(L^2)$, without banding), and the runtime for computing a mutation score to $O(1)$ (from $O(L)$, without banding).

Since we only compute a band of each column, it is natural to avoid \emph{storing} the entire column as well.  We store only the high scoring bands of $A$ and $B$, reducing their storage requirement from $O(L^2)$ to $O(L)$.

Given that the typical template span we consider in Quiver is $\sim$1000bp, these optimizations yield massive performance improvements.  

\section{Evaluation of The Concordance of The Assemblies to The References}

For evaluation of the consensus concordance, we focus on the SNPs and small indel differences between the assembly and the reference to avoid confounding factors like the assembly contig ends may not be trimming correctly. Mummer3 package\cite{Kurtz14759262} is used for alignment and calling the difference.

The following commands are used to calculate the difference. 
 
\begin{verbatim}
nucmer -mum reference.fasta assembly.fa -p asm    # alignment
show-snps -C -x 10 -T -H asm.delta  | wc          # get the number of SNPs
\end{verbatim}

Larger indel differences between our assemblies and the references may not be caught by the above commands. SNPs from alignments with an ambiguous mapping are not reported.

\section{Gene Prediction Evaluation}

For evaluation on predicting genes from an assembly against the same process form a reference genome, we use Prodigal\cite{Hyatt20211023} for gene prediction. We first predict code region sequences from the assembly and the reference genome, then we align the predict coding region sequence to count how many predictions in the assembly have the same predict length in the reference genome. The following commands are used for such assessment:

\begin{verbatim}
prodigal.v2_60.linux -i assembly.fa -d  asm_gene.fa
prodigal.v2_60.linux -i reference.fa -d  ref_gene.fa
blasr asm_gene.fa ref_gene.fa -bestn 5 -m 4 -nproc 24 -out asm_gene.m4
#count how many full length alignments
cat asm_gene.m4 | awk '$8==$7-$6 && $12==$11-$10 {print $2}' | sort -u | wc  
\end{verbatim}
\bibliography{ref}{}
\bibliographystyle{cell}

\end{document}
