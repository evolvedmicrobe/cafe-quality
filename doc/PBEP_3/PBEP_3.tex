%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Stylish Article
% LaTeX Template
% Version 2.0 (13/4/14)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Mathias Legrand (legrand.mathias@gmail.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[fleqn,10pt]{SelfArx} % Document font size and equations flushed left


%----------------------------------------------------------------------------------------
%	Source Code Listings
%----------------------------------------------------------------------------------------
\usepackage{listings}
\usepackage{xcolor}
\definecolor{darkgreen}{rgb}{0.0, 0.2, 0.13}
\definecolor{ao}{rgb}{0.0, 0.5, 0.0}
\lstdefinestyle{sharpc}{language=[Sharp]C, frame=single}
\lstset{% general command to set parameter(s)
basicstyle=\small, % print whole listing small
keywordstyle=\color{blue}\bfseries,
% underlined bold black keywords
commentstyle=\small\color{ao}, % white comments
stringstyle=\ttfamily, % typewriter type for strings
columns=fullflexible, % keeps the comments from exploding
showstringspaces=false} % no special string spaces


% Algorithms package
\usepackage{fancybox}
\usepackage{algorithm}
\usepackage{caption}
\usepackage{algorithmic}
\usepackage{bm}

\usepackage{breqn}

%----------------------------------------------------------------------------------------
%	COLUMNS
%----------------------------------------------------------------------------------------

\setlength{\columnsep}{0.55cm} % Distance between the two columns of text
\setlength{\fboxrule}{0.75pt} % Width of the border around the abstract

%----------------------------------------------------------------------------------------
%	COLORS
%----------------------------------------------------------------------------------------

\definecolor{color1}{RGB}{0,0,90} % Color of the article title and sections
\definecolor{color2}{RGB}{0,20,20} % Color of the boxes behind the abstract and headings

%----------------------------------------------------------------------------------------
%	HYPERLINKS
%----------------------------------------------------------------------------------------

\usepackage{hyperref} % Required for hyperlinks
\hypersetup{hidelinks,colorlinks,breaklinks=true,urlcolor=color2,citecolor=color1,linkcolor=color1,bookmarksopen=false,pdftitle={Title},pdfauthor={Author}}

%----------------------------------------------------------------------------------------
%	ARTICLE INFORMATION
%----------------------------------------------------------------------------------------

\JournalInfo{PBEP \#3, 2015} % Journal information
\Archive{PBEP Series} % Additional notes (e.g. copyright, DOI, review/research article)

\PaperTitle{PacBio Enhancement Proposal \#3 \\{\large Implementing a probability based scoring model for CCS}} % Article title

\Authors{Nigel Delaney and David Alexander} % Authors

\Keywords{} % Keywords - if you don't want any simply remove all the text between the curly brackets
\newcommand{\keywordname}{Keywords} % Defines the keywords heading name

%----------------------------------------------------------------------------------------
%	ABSTRACT
%----------------------------------------------------------------------------------------

\Abstract{ The current PacBio CCS scoring model does not work in probability space, resulting in several oddities and inefficiencies.  This proposal presents a new scoring scheme that, conditioned on QV scores associated with a read, calculates the probability that the read could have generated the derived template by integrating over all possible paths of insertions, deletions, mismatches and matches between the read and the template. }

%----------------------------------------------------------------------------------------

\begin{document}

\flushbottom % Makes all text pages the same height

\maketitle % Print the title and abstract box

\tableofcontents % Print the contents section

\thispagestyle{empty} % Removes page numbering from the first page

%----------------------------------------------------------------------------------------
%	ARTICLE CONTENTS
%----------------------------------------------------------------------------------------

\section{Introduction} % The \section*{} command stops section numbering


There is a long standing tradition in bioinformatics of generating alignments between sequences according to a dynamic programming method that scores alignments according to a fixed set of parameters for insertions, deletions, mismatches and matches.  More sophisticated frameworks treat the alignment problem using a probabilistic modeling framework, such as a Paired HMM model, in which there is a defined generative model with transition probabilities between insertion, deletion and match states.  It has been shown that when considering the optimal alignment, there is a direct correspondence between the probabilistic framework and the standard affine scoring dynamic programming method.\footnote{See the section \textit{The most probable path is the optimal FSA alignment} on page 82 in the book \textit{Biological Sequence Analysis} by Durbin et. al}

Using a probabilistic framework has numerous advantages compared to using a fixed scoring scheme.  Questions such as how much evidence is there for one alignment as compared to another are directly answered by comparing probabilities.  Additionally all the tools and historical research concerning probabilistic modeling become available, guiding the choices of model selection, optimization methods and offering asymptotic guarantees about their accuracy and bias.  Finally, in the context of DNA sequencing probabilistic frameworks allow us to naturally integrate over all the ways a particular read could be generated from a particular template.

The PacBio analysis which generates consensus sequences does not currently use a probabilistic framework, a possibly glaring omission since there is a left-right generative HMM believed to govern how PacBio reads are created.  The central reason for this is that most probabilistic models use fixed parameters for transitions between states, while in practice there are often covariates associated with the bases of an emitted read that indicate how much certainty we have that they are a true representation of the template.

These covariates, typically condensed into QV values, are not easily incorporated into the PacBio probabilistic framework for how reads are generated, and so the statistical method of estimating models and parameters is abandoned in favor of a scoring scheme that rhymes with, though does not actually represent, a probability model.  The central difficulty of QV values is that they are properties of the \textit{read bases} rather than properties of the \textit{template} bases.  In the left-right generative HMM for PacBio data, the model transitions between states are governed by the template, which is the true fixed parameter from which the noisy sequence of reads is recovered.  In contrast, QV values only really make sense if the read is the generative model for the noisy template.  In particular, consider calculating the probability of a template given a read and quality scores, jointly denoted by $R$, using the standard Bayes corrollary.

\begin{dmath}
P(T|R) = \frac{P(R|T) \cdot P(T)}{P(R)}
\end{dmath}

Although our generative Edna model can in theory provide a mechanistic way to calculate the sequence observed in the read data, $R$, there is no generative model for the QV values of $R$ and as a result the needed quantity $P(R|T)$ cannot be calculated.

A possible path forward that would still allow us to use QV values to capture local uncertainty in sections of the reads is to invert the generative model.  That is rather than have a model, like Edna, which generates $R|T$, we envision a left right HMM that generates $T|R$.  By undertaking this reversal of the generative process, we can naturally incorporate QV values as they are once again properties of the `template', rather than of the observed data.  Template selection is then simply a matter of finding $T$ such that we maximize $P(T|R)$.  If the model is calibrated correctly, we can still express our confidence for a given template or variants of it by comparing values of $P(T|R)$.

Although reversing the generative process in this way allows for a natural incorporation of QV values, it also completely inverts the inference process, to the point where we could really be said to be doing probabilistic modeling rather than statistical inference.  In fact, this is exactly what we are doing.  While in the original $P(R|T)$ framework we would use the observed data to calculate the probability of an unknown underlying parameter, $T$, in the new framework we are using known values, the $R$, which are essentially fixed parameters, to calculate the probability of producing data we never observe, $T$.  We are looking for the most probable outcome rather than parameters that best explain the data.

Although this may initially seem like a strange path, there is actually plenty of 

One consequence of this is that it now makes determining these "fixed" parameters, the QV scores, more difficult, as will be discussed later. 

This proposal attempts to create a framework for PacBio consensus scoring that represents a true probability model according to the $P(T|R)$ framework.  It will do this by making the central conceit already mentioned, which is to invert the relationship between the read and the template.  Although we normally think of the read as being a noisy signal of the template and conceive of a generative model for $P(R|T)$, we will instead consider the generative model as the read giving rise to templates, in other words we will consider $P(T|R)$.  

The model proposed for $P(T|R)$ is a left-right generative HMM that is designed to mirror the original generative model for PacBio data, thus approximating its mechanics.  For example, while the original generative model has a `merge` transition which represents the probability that two bases are emitted as one (a homopolymer deletion), the new generative model has a `spit` probability that represents the transition where a base is emitted twice (a homopolymer insertion).  In this way there is a direct correspondence between the `Edna` model and this new probability model.  Given this framework, we can then assess the total probability for $P(T|R)$ by integrating over all paths through the left-right HMM given by the read that would emit the specified template sequence.  The key feature that this model is able to exploit is that the transition between the read bases in the underlying left-right HMM are governed by the QV values that accompany each read base, allowing for them to naturally be accommodated in a generative and probablistic framework.  We now describe this model.


\section{The model for $P(R|T)$}

PacBio emits reads when sequencing a given template and our probabilistic model for this is motivated by our biological understanding of the sequencing process.  This section attempts to explain both the model and the reality it is designed to approximate.

In PacBio sequencing, the enzyme traverses along the template and as it incorporates nucleotides connected to fluorescent molecules, this creates 'pulses' detected by our optical equipment which are converted into basecalls.\footnote{https://www.youtube.com/watch?v=NHCJ8PtYCFc}  Our current model envisions 5 separate processes that can occur as a basepair is being incorporated by a template:


\begin{enumerate}
  \item \textbf{A Correct Emission (Match)} - The basepair is acquired by the enzyme, and a single fluorescent pulse is detected.  Typically, this pulse is identifed as coming from the corresponding fluorophore of the incorporated basepair, but occasionally may be miscalled due to an instrumentation error, which creates a substitution when viewed by later alignments.
  \item \textbf{A Missed Emission (Deletion)} - Sometimes the fluorescent signal emitted during nucleotide incorporation is not enough to meet the criteria to be emitted as a space, leading to a base which was actually incorporated not causing a basepair to be emitted.
  
    \item \textbf{A Merge (Deletion) } - Occasionally, the enzyme will incorporate two basepairs that are only detected as one pulse.  This can occur if either the distance between acquiring separate molecules is too short to be detected, or if the   
   
  
  \item \textbf{A Failed Incorporation (Insertion) } - Occasionally, the enzyme will begin to incorporate a base, but release the bound nucleotide before finishing the incorporation reaction.  This is often referred to as a branch and results in the emission of the same basepair as the true basepair which is next incorporated.
  
  \item \textbf{A Stick (Insertion) } - Sometimes, either the instrument `hiccups' and emits pure noise as a pulse equivalent to a basepair or a fluorescent molecule will randomly float into the detection level of the ZMW and will appear as a signal despite not being connected to an incorporation reaction (this behavior of `sticking' to the bottom of the ZMW leads to the given name).  Although one might think of this event leading to any basepair with equal probability, fluorphores may be quite biased to emit certain basepairs.
     
\end{enumerate}


Although additional complexity may be at play, and if believed to be important should be accounted for, at present these are the dominant processes that occur as the PacBio technology converts basepairs into reads and so for the basis of the approximating HMM model described next.

This HMM 

\begin{figure*}[ht] %% The [h] means place the figure about here.
	\includegraphics[width=\linewidth]{Hmm}
		\caption{The calculation of the deletion score.  The left matrix element is added to an amount determined by the comparison of the bases shown.}				
\end{figure*}

 


%------------------------------------------------

\section{The New Scoring Model and Associated QVs}

  Templates are emitted as paths through the read sequence, and the param

  For example, while 
In doing so we will treat the read and it's associate QV values as fixed parameters, and given several such reads will find the template with the highest probability of occurring across a set of reads.  Although counterintuitive, once framed this way, we are easily able to incorporate the probabilities given by QV values and work in a probabilistic way.  This framework also allows us to cleanly separate the domains of primary and secondary analysis, with the transition between the two demarcated by the paradigm shift  $P(R|T) \rightarrow P(T|R) $ and the concurrent generation of the respective QV values as probabilities of certain events (which can be framed as maximizing the fixed parameters given the observed template). 


Table 1 shows the QV values that will go into the new scoring framework and what each will mean.  Note that, although in a standard context we refer to``deletions" as positions in an alignment where the template has a base but there is no base in the read, because we now view the read as the `reference', the labeling of insertions and deletions has been reversed.

\begin{table*}[bt]
\caption{Table of QV Meanings}
\centering
    \begin{tabular}{| l | p{7cm} | c | c |  }
    \toprule
    \hline
    \textbf{QV Value} &  \textbf{Represents the probability of:} &  \textbf{Notation for probability} &  \textbf{Analogous Edna move}  \\ \hline \hline
    Insertion Different & an insertion prior to this base of a different base than itself. & $p_{id}$  & Dark / null \\ \hline
    Insertion Same & an insertion prior to this base of the same base as itself.& $p_{is}$ & Merge \\ \hline
    Deletion Different & a deletion prior to this base of a different base than itself. & $p_{dd}$  & Stick \\ \hline
    Deletion Same & a deletion prior to this base of the same base as itself. & $p_{ds}$  & Branch \\ \hline
    Substitution & this base being emitted correctly conditioned on it being emitted & $p_{s}$  &  Misclassified \\ \hline
    \bottomrule
    \end{tabular}
\end{table*}





\subsection{ Calculating $\mathbf{P(T|R)}$} 

The inputs are a read of length, J, and a template of length I.  Each base in the read also has an associated QV values given by Table 1.   The algorithm proceeds by recursively populating a matrix and returning the bottom right value as described next.  In describing the algorithm and to account for offsetting, in what follows read and template positions are 1-based indexed, while positions in the matrix that are filled are referred to by 0-based indexes. 


\begin{algorithm}
\caption*{\textbf{Probability Calculation Algorithm}}
\label{calcScore}
\begin{algorithmic}[h]
\FOR{$j=1$ to J}
\FOR{$i=1$ to I}
\IF{ $i =0\ \& j =0 $}
\STATE $A[i,j] \leftarrow 0$
\ELSE
\STATE \[
	A[i,j]  \leftarrow \text{summed}
\begin{cases}
	\text{Match Probability} \\
	\text{Insertion Probability} \\
	\text{Deletion Probability} \\
   	\end{cases}
\]
\ENDIF	
\ENDFOR
\ENDFOR
\RETURN $A[I,J]$
\end{algorithmic}
\end{algorithm}





\subsection{Recursion steps}
After the matrix is initialized, each element in the matrix is recursively populated with the logarithm of the probability of an alignments ending at $i,j$ by examining the available moves, either an insertion, deletion or match.  If a particular step cannot be calculated because it is too close to the edge, it is skipped.  Each possible move is described next.  Top row and first column are initialized as deletions or insertions of the relevant length.  Note that in practice, the elements are stored as log probabilities, but are described as actual probabilities here to simplify the description.






\subsubsection{\textbf{Match}}

\begin{dmath}
\text{Match Probability} = \\
	 \alpha_{i-1,j-1}  \times  (1-p_{id}+p_{is}+p_{dd}+p_{ds})  \times 
	 \begin{cases}
							 \ (1-p_{s}) & \text{if }  R_{j} = T_{i} \\
							 \frac{1}{3} \times p_{s}  & \text{if }  R_{j}  \neq T_{i} 
							 \end{cases}
\end{dmath}


\begin{figure}[ht] %% The [h] means place the figure about here.
	\includegraphics[width=\linewidth]{Match}
		\caption{The calculation of the match score.  The upper element is added to an amount determined by the comparison of the bases shown.}
\end{figure}

This is simply the probability that a match event occurred (rather than an insertion or deletion), plus the appropriate emmission probability for the base (with a uniform probability,  $\frac{1}{3}$, for each possible incorrect base).

\subsubsection{\textbf{Insertion}}


\begin{dmath}
\text{Insertion Probability} = \\
	\alpha_{i-1,j}  \times 
	\begin{cases}
		p_{is_{j+1}}  & \text{if }  T_{i} = R_{j+1} \\
		p_{id_{j+1}} & \text{if }  T_{i}  \neq R_{j+1} 
	\end{cases}
\end{dmath}

\begin{figure}[ht] %% The [h] means place the figure here.
	\includegraphics[width=\linewidth]{Insertion}
		\caption{The calculation of the insertion value.  The upper diagonal element is added to an amount determined by the comparison of the bases shown.}
\end{figure}


This insertion score differs from what is typical.  It is designed to account for two separate events.  First, is designed to account for the possibility that two bases were read simultaneously as one base (presumably because the enzyme acquired the next base before a break in the signal intensity was observed, and if this was the same base, this could be interpreted as one merged pulse).
 
\subsubsection{\textbf{Deletion}}

\begin{figure}[ht] %% The [h] means place the figure about here.
	\includegraphics[width=\linewidth]{Deletion}
		\caption{The calculation of the deletion score.  The left matrix element is added to an amount determined by the comparison of the bases shown.}				
\end{figure}

\begin{dmath}
	\text{Deletion Probability} = \alpha_{i,j-1}  \\
	\times  \begin{cases}
				p_{ds_{j+1}}  & \text{if }  \text{R}_{j} = R_{j+1} \\
				p_{dd_{j+1}} & \text{if }  \text{R}_{j} \neq R_{j+1} 
				\end{cases}
\end{dmath}



The deletion probability, like the insertion probability, has two different values, and as before this is motivated by the underlying process determining how PacBio data is generated.  For PacBio data, there is a ``branching" process that occurs during nucleotide incorporation in which the polymerase may begin to incorporate a base and hold it long enough for the base to be detected in the sequencer before eventually releasing it after failing to complete the incorporation reaction.  In the event of a true branch, the inserted (or in our framework deleted) base should thus match the next template base.  Also as before, the observation could also have been caused by a non-branch event, namely a stick where the base happens to match, and so the QV value used is intended to encompass both a branch and non-branch event leading to this base being emitted.


%------------------------------------------------



\section{Conclusion}

Statistical models should be motivated by the underlying processes that generate the data, but inevitably must make concessions to bias versus variance trade-offs that will govern if in practice the model can perform adequately for the purpose it is designed for.

Our current CCS consensus model is poor largely because it examined this trade-off and sort of lost at both bias and variance.  Variance in parameter estimates, such as the QV recalibration, is introduced because the same observed error, such as a deletion in a homopolymer, could be explained by multiple different events but by using Viterbi scoring only one of these could be considered, leading to multiple modes in the optimization.  Similarly, bias is introduced because in addition to the hard-coded values everywhere, the model does not adequately capture the generative process in which multiple events can lead to the same outcome, forcing it to make artificial trade-offs required when one possibility must represent all possibilities.

This new model is simpler and more unified.  Although considering $P(T|R)$ seems unusual, taking this plunge immediately allows us to maximize the probability of the template sequence given the reads, and provides a statistical framework with which to compare possible consensus sequences.  It can also be seen as an approximation of the Bayes corollarly, which we need to use if we maintained the model for $P(R|T)$ such that we would need to calculate:

\begin{dmath}
P(R|T) = \frac{P(T|R) \cdot P(R)}{P(T)}
\end{dmath}

Although this equation is the fundamentally correct model for primary to optimize the raw read accuracy, the denominator, $P(T)$ is irrelevant if we use a uniform prior over templates, and the probability of the read and it's associated QV values, $P(R)$, is neither defined and almost certainly would be challenging to calculate even if we could define it.  The calculation of $P(R)$ is a fundamental problem that makes modeling difficult.  In contrast, this framework effectively sets $P(R) = 1$ and can be seen as a very strong prior on the observed data (and after all, that read already has been observed to have occurred).  

  Additionally, though the generative model itself has quite changed, the intuition governing the model has been carried over, with every type of move in the generative Edna model having a corresponding QV value and scoring mechanism in the algorithm.  However, nuisances edge cases, such as Deletion Tags and how the separate  `Knight's move' to represent a deletion in a homopolymer context have been excised.   In the later case, the underlying intuition has still been preserved in scoring that captures the branch or merge moves respectively, we have simply replace multiple ways to score these observations with a single one.

Finally, it is worth commenting on how to calculate the QV models in this new framework.  This will be the subject of another write-up, but at present I think it suffices to say that while QV scores are traditionally calculated marginally (that is without any knowledge of the other QVs or there relationship to the true template), one hundred years of statistical literature says that with a unified model it is best to jointly estimate parameters.  In this regard, given any framework we can directly maximize $P(T|R)$, and green pastures likely await there.

\section{Model Training}

As the model specified here meets the criteria for the Cramer-Rao lower bound, maximum likelihood is the best choice for parameter estimation.  Since we are looking to find the ML parameters of a HMM, the Baum-Welch algorithm is a natural choice for optimization.

The central insight in this is that once the alignment is known, the models transition and emission probabilities are easy to estimate by simple counting.  Similarly, once the probabilities of transmission and emission are known, it is easy to find the most likely path by 

\begin{dmath}
\mathcal{L}  (R|T) = P(T|R) = \sum\nolimits_{A} P(T|A,R)P(A|R)
\end{dmath}
\section{Nigel's Notes}

Re-reading older sections, and I dam not sure this can be done without a matrix to store each latent state, which is computationally quite intense.

Another way is to think of the method is in terms of alignments, that is consider this alignment with the given latent states: 

 \begin{center}
 \texttt{MMDMIM}\\
\texttt{ATGC-T}\\
\texttt{AT-CCT}
\end{center}

We can now score this alignment by taking each position and applying the QV values given by the equations above, the probability for the entire alignment is then just the integral over all of these.  This is contrived, but can be workable if we think of the QV values as giving priors for particular alignments, then the probability is just the integral over all possible alignments.

%----------------------------------------------------------------------------------------

\end{document}